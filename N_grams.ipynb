{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# $n$-grams"
      ],
      "metadata": {
        "id": "kVSznS55V5y2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contributors: Nolan, Katie"
      ],
      "metadata": {
        "id": "dwM5pwGIk6f0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook covers $n$-gram language models. Broadly, we'll cover the following concepts:\n",
        "\n",
        "- What are $n$-grams?  \n",
        "- Building a simple $n$-gram model.  \n",
        "- Using an $n$-gram model to make predictions."
      ],
      "metadata": {
        "id": "ZoTTGmQKUfaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to go to **File -> Save a copy in Drive** to make sure your edits are saved."
      ],
      "metadata": {
        "id": "O6SQHRLdVDzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducing $n$-grams\n",
        "> An $n$-gram is an adjacent sequence of length $n$ of characters or words in a given collection of texts, known as a \"corpus.\"\n",
        "\n",
        "For example, given the sentence: `I love ACM Projects!`\n",
        "> The $n$-grams where $n=2$ (bigrams) might look like `[('i', 'love'), ('love', 'acm'), ('acm', 'projects')]`\n",
        "\n",
        "> The $n$-grams where $n=3$ (trigrams) might look like `[('i', 'love', 'acm'), ('love', 'acm', 'projects')]`\n",
        "\n",
        "In this section, you'll learn how to extract $n$-grams. There are a few components to this process:\n",
        "\n",
        "- **Tokenizing** the text: This means identifying all the words, as well as where each sentence starts and ends.  \n",
        "- **Extracting $n$-grams**: given a list of all the tokens in a corpus, identify all the sequences of length $n$.  \n",
        "- **Applying** these components to a larger corpus."
      ],
      "metadata": {
        "id": "y3T2S9zPWGel"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xyhompHeUdSf"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to import packages\n",
        "import seaborn as sns # visualization package\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'  # makes figs nicer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "982a5aef"
      },
      "source": [
        "### 1a. Tokenizing\n",
        "\n",
        "There are many ways to **tokenize** a corpus. Here, we'll opt for a simple approach, which gets rid of all unwanted punctuation (e.g., commas), identifies all the words, and also identifies the *beginning* and *end* of each sentence.\n",
        "\n",
        "- Each unique word token will be represented as an item in a Python list.  \n",
        "- The beginning and end of a sentence will be represented as `<s>` and `</s>`, respectively.\n",
        "\n",
        "**Reflection**: If we're buliding a model of language, why is it useful to identify where sentences tend to begin and end?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you're unfamiliar with Google Colaboratory or Jupyter Notebooks:**\n",
        "This is essentially Google Docs, but for Python code. Every time you connect to Google Colab, you are connected to one of Google's cloud servers with temporary storage. This means that when you disconnect, or stop using Colab for about half an hour, all your cached data here will *disappear.*\n",
        "\n",
        "Also, code here is run iteratively. Instead of IDEs where you run all code written in the file once, you can consider each code block to be portions of the code that can be run separately, but share the same data. For instance, if you run a block of code that does `x=1`, another block of code that does `x=3` will change x to 3. But if you re-run `x=1`, this will set it back to 1. Be careful about using the same name for variables."
      ],
      "metadata": {
        "id": "L1PvgtmbYtEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We'll define our example corpus here:**"
      ],
      "metadata": {
        "id": "vJbxIkkGacu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_corpus = \"The cat chased the mouse. The mouse hid in the wall. The cat could not find the mouse.\""
      ],
      "metadata": {
        "id": "SxGRFdluWgVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The `re` package is used to parse regular expressions.** This basically lets us filter out certain things like punctuation, etc. It's a very involved process and honestly I don't know regex in detail, and you don't necessarily have to either! Here's some more reading to do if you're interested:\n",
        "- https://docs.python.org/3/library/re.html\n",
        "- https://www.w3schools.com/python/python_regex.asp\n",
        "\n",
        "But the two lines you need to know are:\n",
        "```\n",
        "re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
        "```\n",
        "For splitting `text` into sentences, and\n",
        "```\n",
        "re.findall(r'\\b\\w+\\b', sentence.lower())\n",
        "```\n",
        "For splitting the sentence into words."
      ],
      "metadata": {
        "id": "Kgq19FQlXv3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try writing the following - I've written this as pseudocode to try this out.\n",
        "```\n",
        "function tokenize(text):\n",
        "    1. split the text into sentences using re\n",
        "    2. create a list to store all the tokens\n",
        "    3. for each sentence in your text\n",
        "        a. add the start token\n",
        "        b. split the sentence into words using whitespace, remove all alphanumeric characters, and add to tokens\n",
        "        c. add the end token\n",
        "    4. return the list\n",
        "```"
      ],
      "metadata": {
        "id": "cSHShY9QWpcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Your code here\n",
        "def tokenize(text):\n",
        "    # split the text into sentences using re\n",
        "    sentences = ...\n",
        "\n",
        "    # create a list to store all the tokens\n",
        "    ...\n",
        "\n",
        "    # iterate through your sentences using a for loop\n",
        "    # add start token, words (lowercase), and end token - hint: use list method .append() and/or list += [...]\n",
        "    ...\n",
        "\n",
        "    # return list\n",
        "    return ..."
      ],
      "metadata": {
        "id": "AvLbY2XwXlJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this to test:\n",
        "tokenize(\"The man walked to the store.\")"
      ],
      "metadata": {
        "id": "h6ZPmAB5atpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your output should look like\n",
        "\n",
        "> `['<s>', 'the', 'man', 'walked', 'to', 'the', 'store', '</s>']`\n",
        "\n"
      ],
      "metadata": {
        "id": "g4Ya3GKLasvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try running your function on \"small corpus\" here\n",
        "\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "dH6JDPQuazcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65d6783c"
      },
      "source": [
        "### 1b. Extract (and count) $n$-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's start off with a simple example. Using your tokenize funciton:"
      ],
      "metadata": {
        "id": "pHS-NZcAnO6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenize(\"This is a simple test\")"
      ],
      "metadata": {
        "id": "nwTj1jfUnYX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to create bigrams (n-grams with `n = 2`). Slice this into the following bigrams:"
      ],
      "metadata": {
        "id": "h8CLvvf8jPmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_bigram = ...\n",
        "print(\"First bigram:\", first_bigram) # you should see ['this', 'is']"
      ],
      "metadata": {
        "id": "M6O7F7JNjRVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_bigram = ...\n",
        "print(\"Second bigram:\", second_bigram) # you should see ['is', 'a']"
      ],
      "metadata": {
        "id": "0eio46bWjt_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a loop that extracts all bigrams from the tokens list and prints each one"
      ],
      "metadata": {
        "id": "x_nVKTCKj23l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gO8tpzXyj3Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example solution\n",
        "n = 2  # Size of the n-gram (bigram)\n",
        "for i in range(len(tokens) - n + 1):\n",
        "    ngram = tokens[i:i + n]\n",
        "    print(\"Bigram:\", ngram)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jXMQQjHTkV6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see:\n",
        "```\n",
        "Bigram: ['this', 'is']\n",
        "Bigram: ['is', 'a']\n",
        "Bigram: ['a', 'simple']\n",
        "Bigram: ['simple', 'test']\n",
        "```"
      ],
      "metadata": {
        "id": "uC7j7n5kkIvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's convert this into a function to get the number of ngrams given:\n",
        "\n",
        "n, and a list of tokens."
      ],
      "metadata": {
        "id": "bSASQFWCnqIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's pseudocode to refer to:\n",
        "\n",
        "tokens is a list of strings\n",
        "n is an integer for the size of the grams (2 for a bigram, 3 for a trigram, etc.)\n",
        "\n",
        "```\n",
        "def extract_ngrams(tokens, n):\n",
        "1. Create an empty list called ngrams to store each n-gram.\n",
        "2. Loop through the tokens to create n-grams.\n",
        "    We want to create a sequence of n words for each position in the tokens list.\n",
        "    The loop should go from index 0 to (length of tokens) - n.\n",
        "    Think about why this is (length of tokens) - n.\n",
        "    a. Extract a sequence (slice) of n tokens starting at index i.\n",
        "    b. Join the list of words into a single string with spaces between them to store that ngram.\n",
        "    c. Append the resulting string to the ngrams list.\n",
        "3. Create an empty dictionary called ngram_counts to store counts for each n-gram. Keys will be n-gram strings, values will be counts.\n",
        "4. Loop through each n-gram string in the ngrams list.\n",
        "    a. If the ngram is not already in ngram_counts, add it with a count of 1.\n",
        "    b. If the ngram is already present, increase its count by 1.\n",
        "5. Return the dictionary containing all n-gram counts.\n",
        "```"
      ],
      "metadata": {
        "id": "uSWqWNzBb36-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're confused about where to start - refer to [this notebook](https://colab.research.google.com/drive/1kTn2Uk3EvRwLWlegH2SUiQrYW2BmnlIy#scrollTo=jXMQQjHTkV6E) for some help with slicing for n-grams!"
      ],
      "metadata": {
        "id": "Pqg1zZuRl4pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ngrams(tokens, n):\n",
        "    # 1\n",
        "\n",
        "    # 2\n",
        "        # a\n",
        "        # b\n",
        "        # c\n",
        "    # 3\n",
        "\n",
        "    # 4\n",
        "        # a\n",
        "        # b\n",
        "\n",
        "    # 5"
      ],
      "metadata": {
        "id": "tGF5vDjTgoxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cdafc34",
        "outputId": "d36d939d-e16f-44c7-dd42-da906c81cc7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "### Here's an example of how this function would work\n",
        "# output should be 16, try running again to verify\n",
        "ngrams = extract_ngrams(tokens, 2)\n",
        "len(ngrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53dda38b"
      },
      "source": [
        "#### Questions\n",
        "\n",
        "1. How many unique $n$-grams of length 2 are in `small_corpus`? (Hint: use `len` on `ngrams`.)\n",
        "2. What is the most common $n$-gram? Try writing a function to do this."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "ulimJ0SbkpTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb802c62"
      },
      "source": [
        "### 1c. Apply this to a larger corpus.\n",
        "\n",
        "Now, let's apply this to a much larger **corpus** of text: the book *Emma*, by Jane Austen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af8cee47"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "emma = ' '.join(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
        "emma = emma.replace(\"Mr .\", \"Mr\").replace(\"Mrs .\", \"Mrs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1e99f0e",
        "outputId": "601d448d-9cc8-4c37-dcd6-95acff5ec489"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "172495"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "emma_tokens = tokenize(emma)\n",
        "len(emma_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "525f2b4c"
      },
      "source": [
        "#### Questions\n",
        "\n",
        "1. Use `extract_ngrams` to identify all n-grams of length $2$ from `emma_tokens`.\n",
        "2. How many are there? (Use `len`.)\n",
        "3. What is the most common n-gram? What about the second most common?\n",
        "4. Now use `extract_ngrams` to identify all n-grams of length $3$. How many are there? Which is most common?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c620ff0"
      },
      "outputs": [],
      "source": [
        "### Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b02c936"
      },
      "source": [
        "## Part 2: Building a simple $n$-gram model\n",
        "\n",
        "> An **n-gram language model** is a statistical language model, which assigns a probability to some word $w$ as a function of the $(n-1)$ words preceding $w$. In other words, the probability that a token appears in a sentence depends on the $(n-1)$ tokens before it.\n",
        "\n",
        "> For a bigram model, then, this could be written as: $p(w_i | w_{i-1})$.\n",
        "\n",
        "\n",
        "We'll break this down into steps:\n",
        "\n",
        "1. Theoretical foundations.  \n",
        "2. Building a simple *bigram* model.  \n",
        "3. Generalizing to an $n$-gram model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c27317cd"
      },
      "source": [
        "### 2a: Theoretical foundations\n",
        "\n",
        "We want to estimate: $p(w_i | w_{i-1})$\n",
        "\n",
        "Usually, this **conditional probability** is based on the number of times word $w_i$ occurs in a given context, relative to the number of times that *context* appears.\n",
        "\n",
        "For a bigram model, we could write this as follows:\n",
        "\n",
        "$$p(w_i | w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i)}{\\text{Count}(w_{i-1})}$$\n",
        "\n",
        "For example, $p\\text{(dog|the)}$ would be calculated by dividing the number of times \"the dog\" occurs by the number of times \"the\" occurs.\n",
        "\n",
        "$$p\\text{(dog|the)} = \\frac{\\text{Count(the dog)}}{\\text{Count(the)}}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd8e88aa"
      },
      "source": [
        "### 2b: Build a *bigram* model.  \n",
        "\n",
        "Now let's build a **bigram** model. To represent our $n$-grams, we'll use a nested dictionary structure that looks something like this:\n",
        "\n",
        "```python\n",
        "{'the':\n",
        "     {'dog': 5,\n",
        "      'cat': 5,\n",
        "      'person': 10}\n",
        "}\n",
        "```\n",
        "\n",
        "Here, each number represents the number of times that word (e.g., \"dog\") occurs after the word `\"the\"`. Those numbers could be converted to probabilities by dividing them by the *sum* of their values.\n",
        "\n",
        "```python\n",
        "{'the':\n",
        "     {'dog': .25,\n",
        "      'cat': .25,\n",
        "      'person': .5}\n",
        "}\n",
        "```\n",
        "Let's build a bigram model. Before starting, let's get used to dictionaries for bigrams. If you're unfamiliar with Python Dictionaries, they work like hashmaps - lists of lists. Let's try to store each bigram in a dictionary where each key is the first word, and the value is a list of words that follow it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fill this out**"
      ],
      "metadata": {
        "id": "4Ba1-A2govjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [\"I\", \"love\", \"Python\", \"and\", \"Python\", \"loves\", \"me\"]\n",
        "\n",
        "bigram_dict = {}\n",
        "\n",
        "for i in range(len(tokens) - 1):\n",
        "    # get the token at the ith index\n",
        "    key = ...\n",
        "\n",
        "    # get the token after the ith index\n",
        "    next_word = ...\n",
        "\n",
        "    # if the key doesn't exist, add it with an empty list\n",
        "    if key not in bigram_dict:\n",
        "        bigram_dict[key] = []\n",
        "    # append the next word to the list\n",
        "    bigram_dict[key].append(next_word)\n",
        "\n",
        "print(bigram_dict)\n",
        "\n",
        "# Expected output (order of lists may vary):\n",
        "# {'I': ['love'], 'love': ['Python'], 'Python': ['and', 'loves'], 'and': ['Python'], 'loves': ['me']}"
      ],
      "metadata": {
        "id": "Z72qrp-slFKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise: Go back, and modify the bigram dictionary so that each keyâ€™s value is not just a list but a dictionary counting how many times each following word appears.**"
      ],
      "metadata": {
        "id": "hlg5xN8jo9EB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expected output (the order of keys/dictionary entries may differ):\n",
        "# {\n",
        "#   'I': {'love': 2},\n",
        "#   'love': {'Python': 1, 'coding': 1},\n",
        "#   'Python': {'and': 1, 'loves': 1},\n",
        "#   'and': {'Python': 1, 'I': 1},\n",
        "#   'loves': {'me': 1}\n",
        "# }"
      ],
      "metadata": {
        "id": "tGYLcqXEo_dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then put this into a function:"
      ],
      "metadata": {
        "id": "s9AOfrSeqXQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ngram_model(tokens, n):\n",
        "    # 1. Initialize the model as an empty dictionary.\n",
        "    model = {}\n",
        "\n",
        "    # 2. Loop through tokens so that you can extract complete n-grams.\n",
        "    #    The loop will run until there are enough tokens for one full n-gram.\n",
        "    ...\n",
        "        # a. Extract the context (first n-1 tokens) and convert to a tuple.\n",
        "        ...\n",
        "\n",
        "        # b. Extract the next word (the n-th token in the current slice).\n",
        "        ...\n",
        "\n",
        "        # c. If the context is not in the model, add it with an empty dictionary.\n",
        "        ...\n",
        "\n",
        "        # d. If the next_word is not in the inner dictionary for this context, initialize it.\n",
        "        ...\n",
        "\n",
        "        # e. Increment the count for this next_word.\n",
        "        ...\n",
        "\n",
        "    # 3. Return the completed n-gram model.\n",
        "    return model"
      ],
      "metadata": {
        "id": "mHidbQtPqYrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example Solution\n",
        "def build_ngram_model(tokens, n):\n",
        "    # 1. Initialize the model as an empty dictionary.\n",
        "    model = {}\n",
        "\n",
        "    # 2. Loop through tokens so that you can extract complete n-grams.\n",
        "    #    The loop will run until there are enough tokens for one full n-gram.\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        # a. Extract the context (first n-1 tokens) and convert to a tuple.\n",
        "        context = tuple(tokens[i:i+n-1])\n",
        "\n",
        "        # b. Extract the next word (the n-th token in the current slice).\n",
        "        next_word = tokens[i+n-1]\n",
        "\n",
        "        # c. If the context is not in the model, add it with an empty dictionary.\n",
        "        if context not in model:\n",
        "            model[context] = {}\n",
        "\n",
        "        # d. If the next_word is not in the inner dictionary for this context, initialize it.\n",
        "        if next_word not in model[context]:\n",
        "            model[context][next_word] = 0\n",
        "\n",
        "        # e. Increment the count for this next_word.\n",
        "        model[context][next_word] += 1\n",
        "\n",
        "    # 3. Return the completed n-gram model.\n",
        "    return model"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zvFETboLqfzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test on this:"
      ],
      "metadata": {
        "id": "-lyltGUrqlxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"and\", \"the\", \"cat\", \"slept\"]\n",
        "n = 2  # This creates a bigram model (context of 1 word)\n",
        "model = build_ngram_model(tokens, n)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "8ukAMo_bqmVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's make predictions"
      ],
      "metadata": {
        "id": "Z6embn_6quGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function that uses your n-gram model to predict the next word given a specific context. The prediction will be the word that has the highest count for that context."
      ],
      "metadata": {
        "id": "ZNhPxCLmq47m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(model, context):\n",
        "    # 1. Check if the given context exists in the model. If not, return \"No prediction available\"\n",
        "    ...\n",
        "\n",
        "    # 2. Get the dictionary of possible next words for this context.\n",
        "    ...\n",
        "\n",
        "    # 3. Initialize variables to keep track of the best prediction.\n",
        "    best_word = None\n",
        "    highest_count = -1\n",
        "\n",
        "    # 4. Loop through the possible next words and find the one with the highest count.\n",
        "    #    Hint: you can loop through both keys (words) and values (count) using the dictionary method .items()\n",
        "    ...\n",
        "\n",
        "    # 5. Return the best prediction.\n",
        "    return best_word"
      ],
      "metadata": {
        "id": "TmVvtH3yqvWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your code on this\n",
        "# usng a bigram model\n",
        "tokens = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"and\", \"the\", \"cat\", \"slept\"]\n",
        "n = 2\n",
        "model = build_ngram_model(tokens, n)\n",
        "\n",
        "# The context is a tuple with one word (because n=2).\n",
        "context = (\"the\",)\n",
        "prediction = predict_next_word(model, context)\n",
        "print(\"Prediction for context\", context, \"is:\", prediction)"
      ],
      "metadata": {
        "id": "3ugPlollrJnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example Solution\n",
        "def predict_next_word(model, context):\n",
        "    # 1. Check if the given context exists in the model.\n",
        "    if context not in model:\n",
        "        return None  # or return \"No prediction available\"\n",
        "\n",
        "    # 2. Get the dictionary of possible next words for this context.\n",
        "    next_words = model[context]\n",
        "\n",
        "    # 3. Initialize variables to keep track of the best prediction.\n",
        "    best_word = None\n",
        "    highest_count = -1\n",
        "\n",
        "    # 4. Loop through the possible next words and find the one with the highest count.\n",
        "    for word in next_words:\n",
        "        count = next_words[word]\n",
        "        if count > highest_count:\n",
        "            highest_count = count\n",
        "            best_word = word\n",
        "\n",
        "    # 5. Return the best prediction.\n",
        "    return best_word"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jAc1X6kosZHl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}